---
title: "<Draft> Lab3: Reducing Crime"
subtitle: "w203 Lab3"
author: "Harith Elrufaie and Gaurav Desai"
output: pdf_document
---

#Introduction
We have been tasked to help shape up a political campaign in North Carolina. We are equipped with "Crime Statistics" data of year 1987 for selected counties in North Carolina and our task is to decipher this data and understand various factors that could affect the crime rate and make statistics backed suggestions applicable to local government to improve the Crime rate in North Carolina. 


## Setup
First, we load the necessary libraries.
```{r library loads with sppressed warning}
suppressMessages(library(dplyr))
suppressMessages(library(stargazer))
suppressMessages(library(corrplot))
suppressMessages(library(ggplot2))
```

## Data Load
```{r data}
rawCrimeData = read.csv("crime_v2.csv")
dim(rawCrimeData)
summary(rawCrimeData)
str(rawCrimeData)
```
The dataset contains **25** variables and **97** observations. Now lets see if there are any bad data that needs to be cleaned up.

## Data Quality/Clean-up

### Convert county to factor
Since county is not a measurement, it won't make sense to roll it up for aggregation or do any mathematical operation (like taking average) on it. Hence lets convert it into factor.

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData$county <- as.factor(rawCrimeData$county)
length(levels(rawCrimeData$county))
sum(is.na(rawCrimeData$county))
```

Interestingly we have 91 non NA rows but only 90 levels. Eyeballing the data shows there are two identical rows for county 193, same can be verified using duplicated function. Lets drop the duplicate row.
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData[duplicated(rawCrimeData[!is.na(rawCrimeData$county),]), c("county","crmrte")]
#so lets delete the duplicate row
rawCrimeData <- rawCrimeData[!duplicated(rawCrimeData[!is.na(rawCrimeData$county),]),] 
nrow(rawCrimeData) #after removal of duplicate we are left with 96 observations..
```

### Convert prbconv to number
Now lets convert prbconv from factor to number because it is a ratio of convictions to arrest so it is actual measurement and should be stored as number for aggregations and other mathematical operations.
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData$prbconv <- as.numeric(levels(rawCrimeData$prbconv))[rawCrimeData$prbconv]
summary(rawCrimeData$prbconv)
```

### Remove NAs
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
#let us find how many NA records we have..
sum(is.na(rawCrimeData$crmrte))
sum(is.na(rawCrimeData$county))
```
The data set contains 6 NA rows, lets remove them
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
crimeData <- rawCrimeData[!is.na(rawCrimeData$county),]
min(complete.cases(crimeData))
```

# EDA
Now, we'll conduct an Exploratory Data Analysis of the given dataset. This process will help us gain a solid understanding of our variables, which will eventually be essential to choose right variable combinations for our regression model.

## Univariate Analysis
###crmrte: crimes committed per person
This is outcome variable for our regresison model where we will try and derive relation between various independent variables and crime rate.
Looking at the quantiles of crmrte we can see large difference between 3rd quantile and max. So there are few outliers counties with very high crime rates than rest. Same is evident from histogram.
To take care of outliers and fit the variable into normal distribution, lets take a log of crime rate.
But we note that these are crimes rates per person and all the values are between 0 and 1. This range is not suitable for logarithms. 
So lets change the scale by creating new variable for crime rate per 1000 people (crmrtepk) and then lets take log(log_crmrtepk). The new variable is log_crmrtepk which shows nice normal distribution. Going forward whenever we talk about crime rate, we will use log_crmrtepk (log of crmrt per k) 

Also we not the righ most outlier, county=119 has crime rate of 98 for every 1000 people, that is 1 crime per every 10 people which is very high. Population Density also is highest among all counties. More information is required to understand what is so different about this county so that appropriate remedial action can be suggested.
```{r}
summary(crimeData$crmrte)
boxplot(crimeData$crmrte)
hist(crimeData$crmrte)
crimeData$crmrtepk <- crimeData$crmrte * 1000
crimeData$log_crmrtepk <- log(crimeData$crmrtepk)
hist(crimeData$log_crmrtepk)
crimeData[crimeData$crmrtepk>90,c("county","crmrtepk", "density")]
```

## Also convert polpc from per capita to per 1000 people to keep the scale
Since we have converted crimerate from per capita to per K people, lets also convert other per capita variable polpc to same scale. 
While scaling we notice that for county 115 the police per 1000 people is highest at 9 while average is just 1.7. Noteably the second highest police per 1k is 4.5. Crime rate and density in this county is not high, but prbarr is highest at 1.09 and avgsen is highest at 20.7. Which means County 115 has highest police numbers which would logically translate into highest arrests. Though higher police numbers can not logcally explain highest average sentence in that county. We sould need more information about this county, may be there is a central jail for all of western counties of North Carolina which would explain highest police population and highest average sentences.

```{r}
crimeData$polpk <- crimeData$polpc * 1000
summary(crimeData$polpk)
crimeData[crimeData$polpk>4,]
crimeData[crimeData$avgsen>15,]
```

### Check if there are any abnormal probabilities
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
#Now lets see if any of the probability is crossing 0 to 1 range
filter(crimeData, prbarr< 0 | prbarr>1 | 
         prbconv < 0 | prbconv > 1 | 
         prbpris < 0 | prbpris > 1) [,c("county", "prbarr", "prbconv", "prbpris")]
```

We have 10 counties where prbconv is greater than 1, which means there are more convictions than arrests. Infact there is one county=185 which has more than 2 convictions per arrest. Out of these 10 counties, one county (115) also has prbarr greater than 1 indicating more arrests than offences. We have talked about this county in detail while analysing polpc variable earlier.

Under normal curcomstances pribabilities should not cross 0 to 1 range, but in this case the probabilitis are mere proxies to actual police and judicioury data. One of the possible explanation to more convictions than arrest could be transfers of arrested people from base location to newar court locations within or outside of North Carolina.
In absense of more details on these probabilities we keep the probabilities above 1 as it is and proceed further with our analysis

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
data.probabilities <- cbind(crimeData$prbarr,crimeData$prbconv,crimeData$prbpris,deparse.level = 2)
colnames(data.probabilities) <- c("prbarr", "prbconv", "prbpris")
summary(data.probabilities)
```
Now lets look look in detail at outliers in these probabilities. Outlier in prbarr is county 115 which has been already discussed in earlier section for polpc. Lets look at outlier in prbconv which is county 185
```{r}
crimeData[crimeData$prbconv>2,]
summary(crimeData)
```
We observe an interesting combination of extremes for County 185. It has highest Arrest to Conviction ratio of 2.1. At the same time least average sentense of 5.4 days. It has highest % of minority as of 1980 at 64%. And very high weekly wage in service industry at 2177. It is difficult to conclude by such extremes without knowing more about that county. But a best guess would be there are more convictions for small petit crimes for which there are no arrest, may be just community service or warnings. Hence conviction ration is very high while average sentence is lowest. 

### avgsen : Average sentence (in days)
avgsen shows normal distribution with couple of outliers on right. Out of top 3 counties with average sentence, we have already analysed county 115 while analysing polpc. The other two counties 41 and 127 have very high % of minority (42% and 34% respectively). It is difficult to draw conclusion as to why higher average sentence in these areas without any spike in crime rate. Concerned authorities should imvestigate this further.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$avgsen)
hist(crimeData$avgsen, breaks=20, main = "Histogram of avgsen"
     , cex.main=0.8, xlab="avgsen")
crimeData[crimeData$avgsen>15,c("county","avgsen","pctmin80", "crmrtepk")]
```

### density: people per sq. mile
Density distribution is skewed with high concentration between .5 to 1.5 people per sq. mile. But there are ouliers at both end. Lets look at them. 
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$density)
crimeData[crimeData$density<.3 | crimeData$density>7,]

```
We have already talked about county 119 having highest density 8.8 people per square mile. Whereas county 173 has very low density of 0.00002 with highest mix of 0.42 i.e. it has highest % of face o face crimes. The population density is so low that mix could be at its peak even by chance. The population density is unrealistically low hence we replace it with mean of density from rest of the counties
```{r}
crimeData[crimeData$density<.3,]$density <- mean(crimeData[crimeData$density>.3,]$density)
```


### taxpc: tax revenue per capita
Looking at the histogram of probability of sentence, the distribution appears to be positively skewed. Applying `log()` shows the histogram to appear slightly positively skewed. We will also scale this to per 1000 people to bring in line with crime rate. The linear regressions would benefit from this transformation.
The one outlier with 119 taxpc does not show any other extreme value not does it show any super high wages. So this county looks to be wealthy county in general.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
crimeData[crimeData$taxpc>100,]
hist(crimeData$taxpc, breaks=20, main = "Histogram of Tax revenue per capita"
     , cex.main=0.8, xlab="Tax revenue per capta")
hist(log(crimeData$taxpc*1000), breaks=20, main = "Histogram of Log Tax revenue per capita"
     , cex.main=0.8, xlab="Log of Tax revenue per capta")
crimeData$taxpcpk <- log(crimeData$taxpc*1000)
```

### pctmin80: perc. minority, 1980
Looking at the histogram of % of minority as of 1980, it is equally distributed. There are no suprises or any outliers that interests us.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$pctmin80, breaks=20, main = "Histogram of % minority", xlab = "")
```

### mix: offense mix: face-to-face/other
Looking at the histogram, the distribution appears to be slightly positively skewed with few outliers. But otherwise this is fairly normallly distributed. Looking at the top 2 counties for mix are located in the western region. Difficult to draw any conclusion based on this but something for authorities to look into. 
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$mix, breaks=20, main = "Face-to-face/other"
     , cex.main=.8, xlab = "")
summary(crimeData$mix)
crimeData[crimeData$mix>.4,c("county", "west", "central", "urban", "mix")]
```

### pctymle: percent young male
Looking at the histogram, the distribution appears to be positively skewed with a long tail and one distant outlier. 24% young male population might indicate a large manufacturing industry or some sort of labour intesive work setup in this county though manufacturing or any other wage does nto support this deduction. In absense of any other evidence we will keep this outlier without any modification.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$pctymle)
crimeData[crimeData$pctymle>.2,]
hist(crimeData$pctymle, breaks=20, main = "Percent Young Male"
     , cex.main=.8, xlab = "")
hist(log(crimeData$pctymle), breaks=20, main = "Log Percent Young Male"
     , cex.main=.8, xlab = "")
#crimeData <- filter(crimeData, pctymle < .20)
#hist(log(crimeData$pctymle), breaks=20, main = "Log Percent Young Male (Outlier Removed)"
#     , cex.main=.8, xlab = "")
```

### wages
Now lets look at all wages together. We will also calculate average wage across all wage categories.
Overall all wages look well distributed with some spikes in each othe wages.Total wage is almost perfectly normally distributed.
The red line represents average for each of the category. Inrerestingly retal has least of the wages and fed has the higes wage.

```{r}
crimeData$wtotal<-crimeData$wcon+crimeData$wtuc+crimeData$wtrd
                          +crimeData$wfir+crimeData$wser+crimeData$wmfg
                          +crimeData$wfed+crimeData$wsta+crimeData$wloc

wages <- rbind(data.frame(wageType="wcon", wage=crimeData$wcon, meanWage=mean(crimeData$wcon)),
               data.frame(wageType="wtuc", wage=crimeData$wtuc, meanWage=mean(crimeData$wtuc)),
               data.frame(wageType="wtrd", wage=crimeData$wtrd, meanWage=mean(crimeData$wtrd)),
               data.frame(wageType="wfir", wage=crimeData$wfir, meanWage=mean(crimeData$wfir)),
               data.frame(wageType="wser", wage=crimeData$wser, meanWage=mean(crimeData$wser)),
               data.frame(wageType="wmfg", wage=crimeData$wmfg, meanWage=mean(crimeData$wmfg)),
               data.frame(wageType="wfed", wage=crimeData$wfed, meanWage=mean(crimeData$wfed)),
               data.frame(wageType="wsta", wage=crimeData$wsta, meanWage=mean(crimeData$wsta)),
               data.frame(wageType="wloc", wage=crimeData$wloc, meanWage=mean(crimeData$wloc)),
               data.frame(wageType="wtotal", wage=crimeData$wtotal, meanWage=mean(crimeData$wtotal)))

ggplot(wages, aes(x=wage)) + geom_histogram(bins=40, color="white") +
  facet_wrap(~wageType, scales="free") + geom_vline(aes(xintercept=meanWage), color="red")
```

## Analysis of Key Relationships
It is very imperative to realize the relationship between crime rate and all the data available to us. We'll use `corrplot` to make the exploration of key relationships clearer.
```{r}
cex.before <- par("cex")
par(cex=.8)
corrplot(cor(crimeData[ , (names(crimeData) %in% 
                             c("crmrte", "prbarr", "prbconv", "prbpris", "avgsen"
                               , "polpc", "density", "taxpc", "west", "central"
                               , "uraban", "pctmin80", "wcon", "wtuc", "wtrd"
                               , "wfir", "wser", "wmfg", "wfed", "wsta", "wloc"
                               , "mix", "pctymle"))])
               ,tl.pos = "lt", tl.col="black", order="AOE",  number.cex=.5, type="lower"
         , method="number", number.digits=2
               )
par(cex = cex.before)
```

The above plot also indicates the following *positive* relationships with crime rate:
\begin{enumerate}
  \item Probability of Arrest (prbarr)
  \item Probability of Conviction (prbconv)
  \item West region of NC (west)
\end{enumerate}

The above plot indicates the following *negative* relationships with crime rate:
\begin{enumerate}
  \item Density (density).
  \item Tax revenue per capita (taxpc).
  \item All wage variables.
  \item Young Male (pctymle)
\end{enumerate}

### Crimes Committed per person (crmrte) & People per sq. (density)
As you can see from the correlation plot below, there is a positive linear relationship between crime rate and density.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(log(crimeData$density), log(crimeData$crmrte), 
    main="Crime Density vs Crime Rate", 
    xlab="Log Density",
    ylab="Crimes Committed", cex.main=0.8)
abline(lm(log(crimeData$crmrte) ~ log(crimeData$density)))
cor(crimeData$crmrte, crimeData$density)
```

### Crimes Committed per person (crmrte) & Tax revenue per capita (taxpc)
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(log(crimeData$taxpc), log(crimeData$crmrte), 
    main="Tax revenue per capita vs Crime Rate", 
    xlab="Tax revenue per capita",
    ylab="Crimes Committed", cex.main=0.8)
abline(lm(log(crimeData$crmrte) ~ log(crimeData$taxpc)))
cor(crimeData$crmrte, crimeData$taxpc)
```

### Crimes Committed per person (crmrte) & Probabiliy of Arrest (prbarr)
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(log(crimeData$prbarr), log(crimeData$crmrte), 
    main="Probabiliy of Arrest vs Crime Rate", 
    xlab="Probabiliy of Arrest",
    ylab="Crimes Committed", cex.main=0.8)
abline(lm(log(crimeData$crmrte) ~ log(crimeData$prbarr)))
cor(crimeData$crmrte, crimeData$prbarr)
```

### Crimes Committed per person (crmrte) & Tax revenue per capita (prbconv)
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(log(crimeData$prbconv), log(crimeData$crmrte), 
    main="Probablity of Conviction vs Crime Rate", 
    xlab="Probablity of Conviction",
    ylab="Crimes Committed", cex.main=0.8)
abline(lm(log(crimeData$crmrte) ~ log(crimeData$prbconv)))
cor(crimeData$crmrte, crimeData$prbconv)
```

# Proposed Models

## Model 1: with only the explanatory variables
Using a combination of key positive (prbarr, prbconv) and negative attributes (density) to crime rate, we're recommending the following model:

$$crimeDeterm = \beta_0 + \beta_1 \cdot log(density) + \beta_2 \cdot log(prbarr) + \\
\beta_3 \cdot log(prbconv) + \beta_4 \cdot log(pctymle)$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model1 <- lm(log(crmrte) ~ log(density) + log(prbarr) + log(prbconv) 
             + log(pctymle), data=crimeData)
summary(model1)
plot(model1)
```

## Model 2: with key explanatory variables and only covariates
In this model, we'll include the variables (avgsen, mix, taxpc), as we think they will contribute to the accuracy of your results without introducing substantial bias.

$$crimeDeterm = \beta_0 + \beta_1 \cdot log(density) + \beta_2 \cdot log(prbarr) + \\
\beta_3 \cdot log(prbconv) + \beta_4 \cdot log(pctymle) + \beta_5 \cdot log(avgsen) + \beta_6 \cot log(mix) + \\
+ \beta_7 \cdot log(taxpc)$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model2 <- lm(log(crmrte) ~ log(density) + log(prbarr) + log(prbconv) 
             + log(pctymle) + log(avgsen) + log(mix) + log(taxpc), data=crimeData)
summary(model2)
plot(model2)
```

## Model 3: includes the previous covariates, and most, if not all, other covariates
In this model, we'll include all the data available to us to demonstrate the robustness of results to model specification. 

$$crimeDeterm = \beta_0 + \beta_1 \cdot log(density) + \beta_2 \cdot log(prbarr) + \\
\beta_3 \cdot log(prbconv) + \beta_4 \cdot log(pctymle) + \beta_5 \cdot log(avgsen) + \beta_6 \cdot log(mix) + \\
\beta_7 \cdot log(taxpc) +\\
\beta_8 \cdot prbpris + \beta_9 \cdot log(polpc) + \beta_{10} \cdot log(pctmin80) + \beta_{11} \cdot log(wcon)  \\
+ \beta_{12} \cdot log(wtrd) + \beta_{13} \cdot wfir + \beta_{14} \cdot log(wser) + \beta_{15} \cdot log(wmfg)  \\ 
+ \beta_{16} \cdot log(wfed) + \beta_{17} \cdot log(wsta) + \beta_{18} \cdot wloc$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model3 <- lm(log(crmrte) ~ log(density) + log(prbarr) + log(prbconv) 
             + log(pctymle) + log(avgsen) + log(mix) + + log(taxpc) 
             + prbpris + log(polpc) 
             + log(pctmin80) + log(wcon) + log(wtrd) + wfir + log(wser) + log(wmfg)
             + log(wfed) + log(wsta) + wloc, data=crimeData)
summary(model3)
plot(model3)
```

## All 3 Regression models at a glance

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'} 
stargazer(model1, model2, model3, type = "text", title="Comparison of 3 Regression models", float=FALSE)
```

## Omitted Variables
We believe that following omitted variables may contribute towards crime rate regression results.

1. Literacy: Higher the literacy, crime rate should go down. In general terms as literacy increases, it is easier for people to find jobs, which deters them from conducting crimes. 

2. Poverty: If per capita income is not distributed equally then there is high chance of crimes in that area. Tax per capita tries to proxy this variable but it does not capture the high to low distribution of income. If per capita income has huge variance from mean then crime rate should go up. Different wages provided in the data may act as proxy as they cover most of the wage range except may be farming and other self-employed people.

3. Corruption: Higher the corruption, more the crime rate in the area. More corruption generally disrupts employment and effectively pushes people into criminal activity.

4. Historic criminal rate of the area: If previous generation had high criminal rate in a particular area then new generation would grow in that area and continue following same foot steps. So we should also measure this continuity effect. It is much easier for new people to turn to criminals where there are already plenty of established criminals than areas where crime is low.

% population below poverty line



# Conclusion
Our Regression Model (Model 1) indicates that as population density increases and the young male percentage increases, the crime rate grows. So policymakers need to pay attention to more urbanized or highly dense regions with a high male ratio. Also, steps should be taken to improve gender by diversifying the community, for instance bringing more women and men of different age groups, which potentially can bring down crime rate.

More important aspect is the effect of strong arrest and conviction ratio on the crime rate. Having strong and capable police has a noticeable deterrent effect on crime rate. Therefore, policymakers should concentrate on strengthening the police and judiciary system and deter people from committing crimes by setting strong examples of arrests and convictions.
