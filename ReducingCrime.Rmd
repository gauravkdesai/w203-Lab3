---
title: "Lab3: Reducing Crime"
subtitle: "w203 Lab3"
author: "Harith Elrufaie and Gaurav Desai"
output: 
  pdf_document:
      toc: true
---
\pagebreak

#Introduction
We have been tasked to help shape up a political campaign in North Carolina. We are equipped with "Crime Statistics" data of year 1987 for selected counties in North Carolina. Our task is to decipher this data and understand various factors that could affect the crime rate and make statistics backed suggestions applicable to local government to improve the Crime rate in North Carolina. 


## Setup
First, we load the necessary libraries.
```{r library loads with sppressed warning}
suppressMessages(library(dplyr))
suppressMessages(library(stargazer))
suppressMessages(library(corrplot))
suppressMessages(library(ggplot2))
suppressMessages(library(sandwich))
suppressMessages(library(car))
suppressMessages(library(lmtest))
```

## Data Load
```{r data}
rawCrimeData = read.csv("crime_v2.csv")
dim(rawCrimeData)
```
The dataset contains **25** variables and **97** observations. Now lets see if there are any bad data that needs to be cleaned up.

## Data Quality/Clean-up

### Convert county to factor
Since county is not a measurement, it won't make sense to roll it up for aggregation or do any mathematical operation, therefore we'll convert it into factor.

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData$county <- as.factor(rawCrimeData$county)
length(levels(rawCrimeData$county))
sum(is.na(rawCrimeData$county))
```

Interestingly, we have 91 non NA rows but only 90 levels. Eyeballing the data shows there are two identical rows for county 193, same can be verified using duplicated function. Lets drop the duplicate row.
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData[duplicated(rawCrimeData[!is.na(rawCrimeData$county),])
             , c("county","crmrte")]
#so lets delete the duplicate row
rawCrimeData <- rawCrimeData[!duplicated(rawCrimeData[!is.na(rawCrimeData$county),]),] 
nrow(rawCrimeData) #after removal of duplicate we are left with 96 observations..
```

### Convert prbconv to number
Now lets convert prbconv from factor to number because it is a *ratio* of convictions to arrest, so it is actual measurement and should be analyzed as number for aggregations and other mathematical operations.
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
rawCrimeData$prbconv <- as.numeric(levels(rawCrimeData$prbconv))[rawCrimeData$prbconv]
summary(rawCrimeData$prbconv)
```

### Remove NAs
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
#let us find how many NA records we have..
sum(is.na(rawCrimeData$county))
```
The data set contains 6 NA rows, lets remove them
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
crimeData <- rawCrimeData[!is.na(rawCrimeData$county),]
min(complete.cases(crimeData))
```

# Exploratory Data Analysis
Now, we'll conduct an Exploratory Data Analysis of the given dataset. This process will help us gain a solid understanding of our variables, which will eventually be essential to choose right variable combinations for our regression model.

## Univariate Analysis
###crmrte: crimes committed per person
This is outcome variable for our regression model where we will try and derive relationships between various independent variables and crime rate.

Looking at the quarantines of crmrte we can see large difference between 3rd quantile and max. So there are few outliers counties with very high crime rates than rest. This is also evident from histogram.

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$crmrte, breaks=20, main = "Histogram of Crime Rate"
     , cex.main=0.8, xlab="Crime Rate (crmrte)")
```

To take care of outliers and fit the variable into normal distribution, we can easily take a log of crime rate. However, we observed that the values of crimes rates per person are between 0 and 1. This range is not suitable for logarithms. 
Instead, we decided to scale by creating new variable for crime rate per 1000 people (crmrtepk) and then lets take `log(crmrtepk)`. The new variable is log_crmrtepk which shows nice normal distribution. Going forward whenever we talk about crime rate, we will use log_crmrtepk (log of crmrt per k) 

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$crmrte)
crimeData$crmrtepk <- crimeData$crmrte * 1000
crimeData$log_crmrtepk <- log(crimeData$crmrtepk)
hist(crimeData$log_crmrtepk, breaks=20, main = "Histogram of Log of Crime Rate per K"
     , cex.main=0.8, xlab="Log of Crime Rate per K (log_crmrtepk)")
```

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
crimeData[crimeData$crmrtepk>90, c("county","crmrtepk", "density")]
```

Also we noticed the right most outlier, county=119 has crime rate of 98 for every 1000 people, that is 1 crime per every 10 people which is very high. Population Density also is highest among all counties. More information is required to understand what is so different about this county so that appropriate remedial action can be suggested.

### Convert polpc from per capita to per 1000 people to keep the scale
Since we have converted crimerate from per capita to per K people, lets also convert other per capita variable polpc to same scale. 
While scaling we notice that for county 115 the police per 1000 people is highest at 9 while average is just 1.7. Notably the second highest police per 1k is 4.5. Crime rate and density in this county is not high, but prbarr is highest at 1.09 and avgsen is highest at 20.7. Which means County 115 has highest police numbers which would logically translate into highest arrests. Though higher police numbers can not logically explain highest average sentence in that county. We need more information about this county, may be there is a central jail for all of western counties of North Carolina which would explain highest police population and highest average sentences.

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
crimeData$log_polpk <- log(crimeData$polpc * 1000)
hist(crimeData$log_polpk, breaks=20
     , main = "Histogram of Log of Police per K"
     , xlab="Log of Police per K (log_polpk)")
summary(crimeData$polpk)
crimeData[crimeData$polpc>.009,c("county", "polpc", "log_polpk", "avgsen")]
```

### Check if there are any abnormal probabilities
```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
#Now lets see if any of the probability is crossing 0 to 1 range
filter(crimeData, prbarr< 0 | prbarr>1 | 
         prbconv < 0 | prbconv > 1 | 
         prbpris < 0 | prbpris > 1) [,c("county", "prbarr", "prbconv", "prbpris")]
```

We have 10 counties where prbconv is greater than 1, which means there are more convictions than arrests. In fact there is one county=185 which has more than 2 convictions per arrest. Out of these 10 counties, one county (115) also has prbarr greater than 1 indicating more arrests than offences. We have talked about this county in detail while analyzing polpc variable earlier.

Under normal circumstances probabilities should not cross 0 to 1 range, but in this case the probabilities are mere proxies to actual police and judiciary data. One of the possible explanation to more convictions than arrest could be transfers of arrested people from outside counties where they were arrested to court locations within county.
In absence of more details on these probabilities we keep the probabilities above 1 as it is and proceed further with our analysis

```{r fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
data.probabilities <- cbind(crimeData$prbarr,crimeData$prbconv,crimeData$prbpris
                            ,deparse.level = 2)
colnames(data.probabilities) <- c("prbarr", "prbconv", "prbpris")
summary(data.probabilities)
```
Now lets look look in detail at outliers in these probabilities. Outlier in prbarr is county 115 which has been already discussed in earlier section for polpc. Lets look at outlier in prbconv which is county 185
```{r}
crimeData[crimeData$prbconv>2,c("county","prbconv","avgsen","pctmin80","wser")]
```
We observe an interesting combination of extremes for County 185. It has highest Arrest to Conviction ratio of 2.1. At the same time least average sentence of 5.4 days. It has highest % of minority as of 1980 at 64%. And very high weekly wage in service industry at 2177. It is difficult to conclude by such extremes without knowing more about that county. But a best guess would be there are more convictions for small petite crimes for which there are no arrest, may be just community service or warnings. Hence conviction ration is very high while average sentence is lowest. 

### avgsen : Average sentence (in days)
avgsen shows normal distribution with couple of outliers on right. Out of top 3 counties with average sentence, we have already analysed county 115 while analyzing polpc. The other two counties 41 and 127 have very high % of minority (42% and 34% respectively). It is difficult to draw conclusion as to why higher average sentence in these areas without any spike in crime rate. Concerned authorities should investigate this further.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$avgsen)
hist(crimeData$avgsen, breaks=20, main = "Histogram of Average Sentence"
     , cex.main=0.8, xlab="Average Sentence (avgsen)")
crimeData[crimeData$avgsen>15,c("county","avgsen","pctmin80", "crmrtepk")]
```

### density: people per sq. mile
Density distribution is skewed with high concentration between .5 to 1.5 people per sq. mile. But there are outliers at both end. Lets look at them. 
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$density)
crimeData[crimeData$density<.3 | crimeData$density>7,c("county", "density", "mix")]
```
We have already talked about county 119 having highest density 8.8 people per square mile. Whereas county 173 has very low density of 0.00002 with highest mix of 0.42 i.e. it has highest % of face o face crimes. The population density is so low that mix could be at its peak even by chance. The population density is unrealistically low hence we replace it with mean of density from rest of the counties
```{r}
density.mean <- mean(crimeData[crimeData$density>.3,]$density)
crimeData[crimeData$density<.3,]$density <- density.mean
```


### taxpc: tax revenue per capita
Looking at the histogram of tax revenue per capita, the distribution appears to be positively skewed. Applying `log()` shows the histogram to appear slightly positively skewed. We will also scale this to per 1000 people to bring in line with crime rate. The linear regressions would benefit from this transformation.
The one outlier with 119 taxpc does not show any other extreme value nor does it show any super high wages to imply high taxes. So this county looks to be wealthy county in general with population paying high taxes from income outside wages.

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$taxpc, breaks=20, main = "Histogram of Tax revenue per capita"
     , cex.main=0.8, xlab="Tax revenue per capta (taxpc)")
crimeData$log_taxpk <- log(crimeData$taxpc*1000)
hist(crimeData$log_taxpk, breaks=20, main = "Histogram of Log Tax revenue per K"
     , cex.main=0.8, xlab="Log of Tax revenue per K (log_taxpk)")
crimeData[crimeData$taxpc>100,]
```

### pctmin80: perc. minority, 1980
Looking at the histogram of % of minority as of 1980, it is equally distributed. There are no surprises or any outliers that interests us.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$pctmin80, breaks=20, main = "Histogram of % minority", xlab = "")
```

### mix: offense mix: face-to-face/other
Looking at the histogram, the distribution appears to be slightly positively skewed with few outliers. But otherwise this is fairly normally distributed. Looking at the top 2 counties for mix are located in the western region. Difficult to draw any conclusion based on this but something for authorities to look into. 
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
hist(crimeData$mix, breaks=20, main = "Face-to-face/other"
     , cex.main=.8, xlab = "")
summary(crimeData$mix)
crimeData[crimeData$mix>.4,c("county", "west", "central", "urban", "mix")]
```

### pctymle: percent young male
Looking at the histogram, the distribution appears to be positively skewed with a long tail and one distant outlier. 24% young male population might indicate a large manufacturing industry or some sort of labor intensive work setup in this county though manufacturing or any other wage does not support this deduction. In absence of any other evidence we will keep this outlier without any modification.
```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
summary(crimeData$pctymle)
crimeData[crimeData$pctymle>.2,]
hist(crimeData$pctymle, breaks=20, main = "Percent Young Male"
     , cex.main=.8, xlab = "")
```

### wages
Now lets look at all wages together. We will also calculate average wage across all wage categories.
Overall all wages look well distributed.Total wage is almost perfectly normally distributed.
The red line represents average for each of the category. Interestingly retail has least of the wages and fed has the highest wage.

Since we don't find significant difference in any of the wages, going forward we will use wtotal as proxy for various wages to see effect of wage on crime..

```{r}
crimeData$wtotal<-crimeData$wcon+crimeData$wtuc+crimeData$wtrd
                          +crimeData$wfir+crimeData$wser+crimeData$wmfg
                          +crimeData$wfed+crimeData$wsta+crimeData$wloc

wages <- rbind(
  data.frame(wageType="wcon", wage=crimeData$wcon, meanWage=mean(crimeData$wcon)),
  data.frame(wageType="wtuc", wage=crimeData$wtuc, meanWage=mean(crimeData$wtuc)),
  data.frame(wageType="wtrd", wage=crimeData$wtrd, meanWage=mean(crimeData$wtrd)),
  data.frame(wageType="wfir", wage=crimeData$wfir, meanWage=mean(crimeData$wfir)),
  data.frame(wageType="wser", wage=crimeData$wser, meanWage=mean(crimeData$wser)),
  data.frame(wageType="wmfg", wage=crimeData$wmfg, meanWage=mean(crimeData$wmfg)),
  data.frame(wageType="wfed", wage=crimeData$wfed, meanWage=mean(crimeData$wfed)),
  data.frame(wageType="wsta", wage=crimeData$wsta, meanWage=mean(crimeData$wsta)),
  data.frame(wageType="wloc", wage=crimeData$wloc, meanWage=mean(crimeData$wloc)),
  data.frame(wageType="wtotal", wage=crimeData$wtotal, meanWage=mean(crimeData$wtotal)))

ggplot(wages, aes(x=wage)) + 
  geom_histogram(bins=40, color="white") + 
  facet_wrap(~wageType, scales="free") + 
  geom_vline(aes(xintercept=meanWage), color="red")
```

### Geographic Indicators
Lets look at the indicator flags for west and central region and indicator for urban counties. There are 22 counties marked under west region and 34 as central. Rest of the counties are neither in west or central region, so we assume they are in east region of North Carolina. Since west and central are not mutually exclusive we can use them for our regression model as is.
Similarly 8 counties are marked as urban, so we assume rest of the counties as non urban counties. 
```{r fig.align='center', fig.height=8, fig.width=8, fig.show='hold'}
sum(crimeData$west)
sum(crimeData$central)
sum(crimeData$urban)
```

## Analysis of Key Relationships

It is very imperative to realize the relationship between crime rate and all the data available to us. We'll use `corrplot` to make the exploration of key relationships clearer.
```{r  fig.align='center', fig.height=8, fig.width=8, fig.show='hold'}
corrplot.mixed(cor(crimeData[ , (names(crimeData) %in% 
                             c("log_crmrtepk", "prbarr", "prbconv", "prbpris"
                               , "avgsen", "log_polpk", "density", "log_taxpk"
                               , "pctmin80", "wtotal", "mix", "pctymle", "west"
                               , "central", "urban"))]),
               tl.pos = "lt", tl.col="black", order="hclust", number.cex=.9, number.digits=2)
```

We can see *strong positive* correlation (>.30) between crime rate (log_crmrtepk) and population density (density), total wages (wtotal), taxes (log_taxpk) and whether the counties is urban (urban). Which is logical in the sense that as population density increases because of urbanization,  then wages and taxes would go up and so would the crimes rate in that area will increase. Note that Density itself is correlated with total wages, taxes and urbanization, so we can take only one of these variables in our model to avoid multicollinearity

On the opposite side, we can see *strong negative* correlation (< -.30) between crime rate (log_crmrtepk) and probability of arrest (prbarr) and probability of conviction (prbconv). And the two probabilities are not correlated with each other. We also see strong negative correlation with western counties. It indicates lower crimes in western counties.

Apart from these strong correlations, we also have *weak positive* correlation between crime rate and % of minority (pctmin80). The relation is not so strong and hence we need not include in our primary model.

Apart from effect on crime rate, there are some other interesting relations that can be seen here. For instance, the number of police per capita (log_polpk) increases as taxes (log_taxpk) and population density increases. And, as police force strengthens the Average sentence (avgsen) goes up. Maybe the additional police force catches serious criminals who get longer duration sentences?

There is another interesting trio of relationships. As mix of face to face crimes go up the probability of arrest goes up but probability of conviction goes down. Logical explanation of this situation would be since there are more face to face crimes, it is easier to identify the person involved and hence more and may be faster arrests, but these extra arrest do not translate to convictions and hence they drag down the conviction rate.

# Proposed Models

## Model 1: with only the explanatory variables
As observed during our EDA, probability of arrest (prbarr), probability of conviction (prbconv), density (density) and whether the county is in western region (west) show largest effect on crime rate (log_crmrtepk). Therefore, it is logical to include those variables in our model. 

Although it is tempting to include log_polpk, we decided not to include it. We found it illogical to say crime rate increases as police per capita increases, whereas the reality is other way round, that is, police per capita increases as crime rate increases. 

We have also considered (log_taxpk, pctymle) for this model, but we concluded that none of these is statistically significant.

Given all of the above, we're recommending the following model:

$$model1 = \beta_0 + \beta_1 \cdot density + \beta_2 \cdot prbarr + \beta_3 \cdot prbconv + \beta_4 \cdot west$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model1 <- lm(log_crmrtepk ~ density + prbarr + prbconv + west, data=crimeData)
summary(model1)$coefficients[,"Estimate"]
summary(model1)$cov.unscaled
summary(model1)$adj.r.squared
```

The model fits fir 67.4% of the population. This is fairly good fit.
The crime rate is positively proportional to density while inversely proportional to rest of the explanatory variables.
We also note that none of the explanatory variables is highly correlated with any other explanatory variable.

Now lets look at the coefficients for their practical significance.
Every unit increase in density results in approximately 1% increase in crime rate.
If we increase the probability of arrest by by 1% then crime rate would decrease by approximately 12% (1/100 of coefficient).
Similarly for 1% increase in conviction ration will decrease crime rate by 5% (1/100 of coefficient)
If the county is in western region then crime rate is 37% lesser than average crime rate.


## Model 2: with key explanatory variables and only covariates
In this model, we'll include the variables (avgsen, mix, pctymle), as we think they will contribute to the accuracy of your results without introducing substantial bias. These variables show good degree of correlation with crime rate, as well as, linear relationship as observed in the EDA. Also these variables do not show high correlation with any of the model 1 explanatory variables so there is less chance of multicolinearity. 

We also considered other variables such as taxpc, but because of the non-linear relationship with crime rate and the clear violation to the Linearity assumption, we decided not to include it.

pctmin80 was also considered for this model. Although it is statistically significant, we realized that a negative correlation between western counties and pctmin80.


$$crimeDeterm = \beta_0 + \beta_1 \cdot density + \beta_2 \cdot prbarr + \beta_3 \cdot prbconv + \beta_4 \cdot west \\
 + \beta_5 \cdot avgsen + \beta_6 \cdot mix + \beta_7 \cdot pctymle$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model2 <- lm(log_crmrtepk ~ density + prbarr + prbconv + west 
             + avgsen + mix + pctymle, data=crimeData)
summary(model2)$coefficients[,"Estimate"]
summary(model2)$cov.unscaled
summary(model2)$adj.r.squared
```

We see slight increase in model fit. We are able to fit 68% of our population by adding 3 new covariates into model 1.
But at the same time we see that some of the new covariates added in model 2 have high covariance with existing variables. 

## Model 3: includes the previous covariates, and most, if not all, other covariates
In this model, we'll include all the data available to us to demonstrate the robustness of results to model specification. 

$$crimeDeterm = \beta_0 + \beta_1 \cdot density + \beta_2 \cdot prbarr + \beta_3 \cdot prbconv + \beta_4 \cdot west \\
 + \beta_5 \cdot avgsen + \beta_6 \cdot mix + \beta_7 \cdot pctmyle \\
 + \beta_8 \cdot pctmin80 + \beta_9 \cdot log\_taxpk + \beta_{10} \cdot urban + \beta_{11} \cdot central \\
 + \beta_{12} \cdot prbpris + \beta_{13} \cdot log\_polpk + \beta_{14} \cdot wtotal$$

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model3 <- lm(log_crmrtepk ~ density + prbarr + prbconv + west
             + avgsen + mix + pctymle + pctmin80 + log_taxpk + urban + central
             +  + prbpris  + log_polpk + wtotal, data=crimeData)
summary(model3)$coefficients[,"Estimate"]
summary(model3)$adj.r.squared
```
The adjusted R square has jumped to 81% indicating the all inclusive model3 is able to predict 81% of the population. But we expect a lot of multicollinearity and overlap between various variables making it difficult to identify true effect of any one variable on crime rate.

## All 3 Regression models at a glance

```{r results = "asis"} 
cov1 <- vcovHC(model1)
robust.se1 <- sqrt(diag(cov1))
cov2 <- vcovHC(model2)
robust.se2 <- sqrt(diag(cov2))
cov3 <- vcovHC(model3)
robust.se3 <- sqrt(diag(cov3))
robust.se <- list(robust.se1, robust.se2, robust.se3)

stargazer(model1, model2, model3
          ,dep.var.labels = "Log of Crime Rate per 1000 People"
          ,covariate.labels = c("Probability of Arrest", "Probability of Conviction"
                                ,"Population Density", "Is Western County"
                                , "Average Sentence", "Face to Face Crime %"
                                , "% of Male", "% of Minority"
                                , "Log of tax per K", "Is Urban County", "Is Central County"
                                , "Probability of Prison", "Log of Police per K"
                                , "Total Wage")
          ,order = c("prbarr", "prbconv", "density", "west"), single.row = TRUE
          ,title="Comparison of 3 Regression models", float=FALSE, header = FALSE, report="vc*sp"
          ,star.cutoffs = c(.05, .01, .001), se = robust.se)
```

The above summary demonstrates the following:

1. `prbarr` and `prbconv` have the best estimates in all three models.

2. `density` coefficient drastically changes in model 3 because variables like `wtotal` and `log_polpk` distribution are synonymous to increase in density and they absorb some of the causality of density.

3. The addition of `pctmin80` in model 3 reduces the effect of Is Western County as western counties have very low minority population hence `pctmin80` absorbs some of the causal effect of west variable in model 3.

4. Adding additional variables to model 2 could have increased the R squared but we'll introduce violations to CLM assumptions.

5. Although Model 3 has the highest R square, CLM assumptions such as linearity and multicollinearity are violated.

## CLM Assumptions Analysis

### Model1 CLM Assumptions Analysis

#### Assumption.1 - Zero Conditional Mean

We’ll now plot model1 in order to assess if the model has zero conditional mean.

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(model1, which=c(1, 5))
```

Looking at the above plots we observe the following:

1. The residual vs fitted indicates that while the red spline line remains close to 0, there is a slight dip and rise and both ends, which may be due to some outlier observations.

2. The residual vs leverage indicates that there are outlier (#51 and #53 to be precise). Observation #51 in particular, is just under Cook's distance of 1, which is concerning. We analyzed what is special about it and realized county 115 has highest values for three variables - police per capita, density and average sentence. There is something special about this county to show highest values in 3 separate explanatory variables. Our recommendation would be to remove this observation to reduce the effect of this outlier on our regression model. Since cook's distance does not cross critical value of 1 the assumption is not violated.

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
model1.no.outliers <- lm(model1, data = crimeData[-51, ])
plot(model1.no.outliers, which=c(1, 5))
```

Given #1 and #2, we're confident to say this assumption is met.

#### Assumption.2 - Linear in Parameters

Looking at the above residual vs leverage plot, we can say the assumption is met.

#### Assumption.3 - Random Sampling

It is not clear to us how the dataset was collected, but we only know it is from 90 counties. Given that North Carolina has 100 counties, it makes us believe this is good enough sampling to consider this assumption as met.

#### Assumption.4 - Multicollinearity

To test this assumption, we'll run the `vif` command

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
vif(model1)
```

Given the small values (less than 5) for all the variables, we'll consider this assumption is met.

#### Assumption.5 - Homoskedasticity

Lets run Breusch-Pagan test to verify Homosckedasticity

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
bptest(model1)
```

Because of large p value (> 0.05), our null hypothesis that model is Homosckedastic can not be rejected. So the assumption is met.

#### Assumption.6 - Normality of Residuals
We will look at the QQ-plot to assess the normality of residuals.

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(model1, which=2)
```

We see a beautiful solid line with few outliers at each end, but we're still considering this condition is met.

### Model2 CLM Assumptions Analysis

#### CLM.1 - Zero Conditional Mean

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(model2, which=c(1, 5))
```

In the residual vs fitted plot we can observe a nice red spline line remains close to 0, there is a slight dip in one side, but it is not significant. The assumption is met. 

#### CLM.3 - Linear in Parameters

Looking at the above residual vs leverage plot, we can say the assumption is met.

#### CLM.3 - Random Sampling

It is not clear to us how the dataset was collected, but we only know it is from 90 counties. Given that North Carolina has 100 counties, it makes us believe this is good enough sampling to consider this assumption as met.

#### CLM.4 - Multicollinearity

To test this assumption, we'll run the `vif` command

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
vif(model2)
```

Given the small values for all the variables, we'll consider this assumption is met.

#### CLM.5 - Homoskedasticity

Lets run Breusch-Pagan test to verify Homosckedasticity

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
bptest(model2)
```

Because of large p value (> 0.05), our null hypothesis that model is Homosckedastic can not be rejected. So the assumption is met.

#### CLM.6 - Normality of Residuals
We will look at the QQ-plot to assess the normality of residuals.

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(model2, which=2)
```

We can see a clear normality proof line.

### Model3 CLM Assumptions Analysis

#### CLM.1 - Zero Conditional Mean

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
plot(model3, which=c(1, 5))
```

In the residual vs fitted plot we can observe a nice red spline line remains close to 0, there is a slight dip in one side, but it is not significant. The assumption is met. 

#### CLM.3 - Linear in Parameters

Looking at the above residual vs leverage plot, we can say the assumption is met.

#### CLM.3 - Random Sampling

Similar to model1 and model2, we are not clear to us how the dataset was collected, but we only know it is from 90 counties. Given that North Carolina has 100 counties, it makes us believe this is good enough sampling to consider this assumption as met.

#### Assumption.4 - Multicollinearity

To test this assumption, we'll run the `vif` command

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
vif(model3)
```

density coefficient is 4.8 which is very close to being termed as high value. This indicates there is multicollinearity between density and another variable added in model 3. This was evident from our earlier correlation matrix analysis as well. Density shows high correlation with 'urban' and 'wtotal' variables. This is logical as urbanization accelerates, population density increases and wages grow.

#### Assumption.5 - Homoskedasticity

Lets run Breusch-Pagan test to verify Homosckedasticity

```{r  fig.align='center', fig.height=3, fig.width=3, fig.show='hold'}
bptest(model3)
plot(model3, which = 3)
```

The p-values of < 0.05 indicates we can reject the null hypothesis that model is Homosckedastic i.e. model is Heteroskedestic. Same can be concluded by looking at Scale-Location plot where we can see residual variance range varies across fitted-values axes.

## Omitted Variables
We believe that following omitted variables may contribute towards crime rate regression results.

1. Literacy: Higher the literacy, crime rate should go down. In general terms as literacy increases, it is easier for people to find jobs, which deters them from conducting crimes. 
Literacy can be measured by number of years of education per capita. This variable should have negative correlation with crime rate and positive correlation with tax per capita and wages.

2. Poverty: If per capita income is not distributed equally then there is high chance of crimes in that area. Tax per capita tries to proxy this variable but it does not capture the high to low distribution of income. If per capita income has huge variance from mean then crime rate should go up. Different wages provided in the data may act as proxy as they cover most of the wage range except may be farming and other self-employed people.

3. Corruption: Higher the corruption, more the crime rate in the area. More corruption generally disrupts employment and effectively pushes people into criminal activity. It is difficult to measure corruption by observing any statistical figure. Only way to measure corruption is by conducting surveys and gathering public feedback. Corruption should have negative correlation with crime rate and tax per capita.

4. Historic criminal rate of the area: If previous generation had high criminal rate in a particular area then a next generation person being raised in that area has higher scope and encouragement to follow the same foot steps. So we should also measure this continuity effect. Of course there will always be exceptions and outliers in this measurement but crime rate is not something that spikes up or down rather it grows with time or comes down with time. So if we get 5 year, 10 year etc. time period average crime rates for that county we can better estimate future crime rate of a county and advise correctly to policy makers.

# Conclusion
Our Regression Model (Model 1) indicates that as population density increases the crime rate goes up. The model also tells us that western counties have significantly lower (37%) crime rate than rest of the North Carolina. So policymakers need to pay attention to more urbanized or highly dense regions specially outside western region.

More important aspect is the effect of strong arrest and conviction ratio on the crime rate. Having strong and capable police has a noticeable deterrent effect on crime rate. Therefore, policymakers should concentrate on strengthening the police and judiciary system and deter people from committing crimes by setting strong examples of arrests and convictions. Between the two factors, 1 % increase in probability of arrest has higher impact on crime rate reduction vs 1 % increase in probability of conviction (12% vs 5%). So if policymakers have to choose one out of two due to budgetary or any other constraints then they should first look at strengthening the police force to increase arrest.

Apart from our regression model, we have following suggestions that may help designing political campaign policies:

1. There is a strong correlation between crime rate and the probability of arrest and conviction. The campaign should focus on counties with the lowest number of arrests and convictions.

2. County 119 has 1 crime per every 10 people.The county also has highest population density. A detailed analysis is required to understand what are the causes of such a high crime rate. Possibly one or more of the omitted variables has a role on this.

3. We observe some drastic parameters for county 185. It has highest arrest to conviction ratio but least average sentence while having highest minority population. The county also shows very high weekly wage in service industry. This raises an alarm. It indicates lot of people are getting convicted but for smaller duration. The fact that county has highest population of minorities, policy makers need to be vigilant and ensure that minority is not harassed or abused. We need to check if there are lot of illegal immigrants working in service industry in this county. Also we need to verify if convictions are valid or law has been abused. If convictions are valid but for petite crimes then citizens should be educated about such crimes so that we can decrease the crimes and the arrest. 
